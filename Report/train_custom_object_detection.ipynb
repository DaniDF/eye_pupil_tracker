{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKfVLK7BLECB"
      },
      "source": [
        "#Local actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSjN6UBFiXgd"
      },
      "source": [
        "##Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFHVi1iziW9y"
      },
      "outputs": [],
      "source": [
        "!mkdir TensorFlow1\n",
        "!virtualenv TensorFlow1\n",
        "!cd TensorFlow1\n",
        "!source bin/activate\n",
        "!git clone 'https://github.com/tensorflow/models.git'\n",
        "!pip install tensorflow\n",
        "!pip install tf_slim\n",
        "!pip install pycocotools\n",
        "!pip install pandas\n",
        "!pip install tf-models-official\n",
        "!pip install opencv-python\n",
        "!pip install lvis\n",
        "!pip install tensorflow_io\n",
        "#!pip install tensorflow tf_slim pycocotools pandas tf-models-official opencv-python lvis tensorflow_io\n",
        "#!pip install tensorflow-object-detection-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL2Je_kPptc3"
      },
      "outputs": [],
      "source": [
        "!mkdir workspace\n",
        "!mkdir workspace/scripts\n",
        "!mkdir workspace/scripts/preprocessing\n",
        "!mkdir workspace/training_demo\n",
        "!cd workspace/training_demo/\n",
        "!mkdir annotations\n",
        "!mkdir exported_models\n",
        "!mkdir images\n",
        "!mkdir images/train\n",
        "!mkdir images/test\n",
        "!mkdir models\n",
        "!mkdir pre-trained-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZoFTrZSUDQI"
      },
      "outputs": [],
      "source": [
        "!cd Tensorflow1/models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!pip install --use-feature=2020-resolver .\n",
        "\n",
        "#Test\n",
        "python object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQTcVR5Wq_8b"
      },
      "source": [
        "###Dowload or create dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLMq5ZJ2rHx3"
      },
      "source": [
        ">Download from [Open Image](https://www.v7labs.com/open-datasets/open-image-v6) and [Kaggle](https://www.kaggle.com/datasets/pavelbiz/eyes-rtte) the eyes and pupils datasets.\n",
        "\n",
        ">Put the train and the test images into <code>Tensorflow1/workspace/training_demo/images/train</code> and <code>Tensorflow1/workspace/training_demo/images/test</code>.\n",
        "\n",
        ">Make sure that train and test folder contain <code>*.xml</code> files for each image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We used [labelImage](https://github.com/tzutalin/labelImg): a graphical image annotation tool and label object bounding boxes in images, very useful because it generated for each images the equivalent information (bounding boxes points xmin, ymin, xmax, ymax and the name of objects) in <code>*.xml</code>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUmDkcyoii4C"
      },
      "source": [
        "###Create label file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyc79g3Dr7O0"
      },
      "source": [
        ">Create the file <code>Tensorflow1/workspace/training_demo/annotations/label_map.pbtxt</code> as following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "item {\n",
        "    id: 1\n",
        "    name: 'Human eye'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Same <code>Tensorflow1/workspace/training_demo/annotations/label_map.pbtxt</code> for the pupil detection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "item {\n",
        "    id: 1\n",
        "    name: 'pupil'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neeKmujTuvy0"
      },
      "source": [
        "###Convert XML files into TFRecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boalzxAgz7W9"
      },
      "outputs": [],
      "source": [
        "!cp -r 'Tensorflow1/models/research/object_detection' 'Tensorflow1/workspace/scripts/preprocessing'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PIxHmx68A4p"
      },
      "source": [
        ">In directory <code>Tensorflow1/workspace/scripts/preprocessing</code> execute (fill the variables with the correct paths):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0Mki3GPvqpx"
      },
      "source": [
        ">Download the utility script to <code>Tensorflow1/workspace/scripts/preprocessing</code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbBskXyqu0xH"
      },
      "outputs": [],
      "source": [
        "!curl 'https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/_downloads/da4babe668a8afb093cc7776d7e630f3/generate_tfrecord.py' --output generate_tfrecord.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD08_llexfh4"
      },
      "source": [
        ">In the same directory execute the downloaded script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf_dCCtt8OAA"
      },
      "outputs": [],
      "source": [
        "!python generate_tfrecord.py -x $PATH_TO_IMAGES_FOLDER/train -l $PATH_TO_ANNOTATIONS_FOLDER/label_map.pbtxt -o $PATH_TO_ANNOTATIONS_FOLDER/train.record\n",
        "!python generate_tfrecord.py -x $PATH_TO_IMAGES_FOLDER/test -l $PATH_TO_ANNOTATIONS_FOLDER/label_map.pbtxt -o $PATH_TO_ANNOTATIONS_FOLDER/test.record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffeeKtRf8-q5"
      },
      "source": [
        "##Configure pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw8fiE2q9F3g"
      },
      "source": [
        ">Download the model from [TensorFlow 2 model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeAiXR9j9l84"
      },
      "source": [
        ">Extract the downloaded model into <code>Tensorflow1/workspace/training_demo/pre-trained-models</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhO1zI3g-td4"
      },
      "source": [
        ">Create the folder according to the downloaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxDx2p2U9FX0"
      },
      "outputs": [],
      "source": [
        "!mkdir Tensorflow1/workspace/training_demo/models/ssd_mobilenet_v2_fpn_keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Configure correctly the <code>pipeline.config</code> file:\n",
        "* num_classes = x, where x is the total number of item for object detection;\n",
        "* The batch size is the amount of samples you feed in your network;\n",
        "* path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "```\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1 #TO CHANGE <---------------------------------\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 640\n",
        "        width: 640\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: \"ssd_mobilenet_v2_fpn_keras\"\n",
        "      depth_multiplier: 1.0\n",
        "      min_depth: 16\n",
        "      conv_hyperparams {\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 3.9999998989515007e-05\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          random_normal_initializer {\n",
        "            mean: 0.0\n",
        "            stddev: 0.009999999776482582\n",
        "          }\n",
        "        }\n",
        "        activation: RELU_6\n",
        "        batch_norm {\n",
        "          decay: 0.996999979019165\n",
        "          scale: true\n",
        "          epsilon: 0.0010000000474974513\n",
        "        }\n",
        "      }\n",
        "      use_depthwise: true\n",
        "      override_base_feature_extractor_hyperparams: true\n",
        "      fpn {\n",
        "        min_level: 3\n",
        "        max_level: 7\n",
        "        additional_layer_depth: 128\n",
        "      }\n",
        "    }\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "        use_matmul_gather: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      weight_shared_convolutional_box_predictor {\n",
        "        conv_hyperparams {\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 3.9999998989515007e-05\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            random_normal_initializer {\n",
        "              mean: 0.0\n",
        "              stddev: 0.009999999776482582\n",
        "            }\n",
        "          }\n",
        "          activation: RELU_6\n",
        "          batch_norm {\n",
        "            decay: 0.996999979019165\n",
        "            scale: true\n",
        "            epsilon: 0.0010000000474974513\n",
        "          }\n",
        "        }\n",
        "        depth: 128\n",
        "        num_layers_before_predictor: 4\n",
        "        kernel_size: 3\n",
        "        class_prediction_bias_init: -4.599999904632568\n",
        "        share_prediction_tower: true\n",
        "        use_depthwise: true\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      multiscale_anchor_generator {\n",
        "        min_level: 3\n",
        "        max_level: 7\n",
        "        anchor_scale: 4.0\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        scales_per_octave: 2\n",
        "      }\n",
        "    }\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 9.99999993922529e-09\n",
        "        iou_threshold: 0.6000000238418579\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "        use_static_shapes: false\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    loss {\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      classification_loss {\n",
        "        weighted_sigmoid_focal {\n",
        "          gamma: 2.0\n",
        "          alpha: 0.25\n",
        "        }\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    encode_background_as_zeros: true\n",
        "    normalize_loc_loss_by_codesize: true\n",
        "    inplace_batchnorm_update: true\n",
        "    freeze_batchnorm: false\n",
        "  }\n",
        "}\n",
        "train_config {\n",
        "  batch_size: 4 #TO CHANGE <------------------------------------------\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    random_crop_image {\n",
        "      min_object_covered: 0.0\n",
        "      min_aspect_ratio: 0.75\n",
        "      max_aspect_ratio: 3.0\n",
        "      min_area: 0.75\n",
        "      max_area: 1.0\n",
        "      overlap_thresh: 0.0\n",
        "    }\n",
        "  }\n",
        "  sync_replicas: true\n",
        "  optimizer {\n",
        "    momentum_optimizer {\n",
        "      learning_rate {\n",
        "        cosine_decay_learning_rate {\n",
        "          learning_rate_base: 0.07999999821186066\n",
        "          total_steps: 50000\n",
        "          warmup_learning_rate: 0.026666000485420227\n",
        "          warmup_steps: 1000\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.8999999761581421\n",
        "    }\n",
        "    use_moving_average: false\n",
        "  }\n",
        "  fine_tune_checkpoint: \"pre-trained-models/ssd_mobilenet_v2_fpn_keras/checkpoint/ckpt-0\"  #TO CHANGE  <------------------------------\"\n",
        "  num_steps: 50000\n",
        "  startup_delay_steps: 0.0\n",
        "  replicas_to_aggregate: 8\n",
        "  max_number_of_boxes: 100\n",
        "  unpad_groundtruth_tensors: false\n",
        "  fine_tune_checkpoint_type: \"detection\"  #TO CHANGE  <-------------------\n",
        "  use_bfloat16: false  #TO CHANGE false = no tpu / true = si tpu  <----------\n",
        "  fine_tune_checkpoint_version: V2\n",
        "}\n",
        "train_input_reader: {\n",
        "  label_map_path: \"annotations/label_map.pbtxt\"  #TO CHANGE  <----------------\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"annotations/train.record\"  #TO CHANGE  <-----------------\n",
        "  }\n",
        "}\n",
        "eval_config {\n",
        "  metrics_set: \"coco_detection_metrics\"\n",
        "  use_moving_averages: false\n",
        "}\n",
        "eval_input_reader: {\n",
        "  label_map_path: \"annotations/label_map.pbtxt\"  #TO CHANGE  <-----------------\n",
        "  shuffle: false\n",
        "  num_epochs: 1\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"annotations/test.record\"  #TO CHANGE  <-----------------\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VaSoVIZGKic"
      },
      "source": [
        "##Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r3oY-EOHAPY"
      },
      "source": [
        ">First copy the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvU7580qG4vs"
      },
      "outputs": [],
      "source": [
        "!cp Tensorflow1/models/research/object_detection/model_main_tf2.py Tensorflow1/workspace/training_demo/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGR-fA1JHJkk"
      },
      "source": [
        ">Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQXiVdtMGa8R"
      },
      "outputs": [],
      "source": [
        "!python model_main_tf2.py --model_dir=models/ssd_mobilenet_v2_fpn_keras/ --pipeline_config_path=models/ssd_mobilenet_v2_fpn_keras/pipeline.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensorboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">We can graphically tracking the different values of our neural network with a Tensorflow tools named Tensorboard, in path: <code>/workspace/training_demo</code> execute the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tensorboard --logdir=models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![learning_rate](LatexPdf/img/tensorboard/learning_rate_both.png)\n",
        "![loss_localization](LatexPdf/img/tensorboard/loss_localization_both.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p0Qhf2OtDKD"
      },
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q46H_nGUtOtF"
      },
      "source": [
        ">We can graphically observate our results on an image directly using shell. We receive in ouput the input img we choose with the object detections boxed. Matplotlib is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVva5wDyubdp"
      },
      "outputs": [],
      "source": [
        "!pip install python-tk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Python <code>test.py</code> script to use TkAgg's GUI Framework. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib as mpl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "mpl.use('TkAgg')\n",
        "\n",
        "CUSTOM_MODEL_NAME = 'training'\n",
        "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpn_keras'\n",
        "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz'\n",
        "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
        "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
        "\n",
        "paths = {\n",
        "    'WORKSPACE_PATH': os.path.join('workspace','training_demo'),\n",
        "    'SCRIPTS_PATH': os.path.join('scripts'),\n",
        "    'APIMODEL_PATH': os.path.join('models'),\n",
        "    'ANNOTATION_PATH': os.path.join('workspace','training_demo','annotations'),\n",
        "    'IMAGE_PATH': os.path.join('.'),\n",
        "    'MODEL_PATH': os.path.join('workspace','training_demo','models'),\n",
        "    'PRETRAINED_MODEL_PATH': os.path.join('workspace','training_demo','pre-trained-models'),\n",
        "    'CHECKPOINT_PATH': os.path.join('workspace','training_demo','models',CUSTOM_MODEL_NAME), \n",
        "    'OUTPUT_PATH': os.path.join('workspace','training_demo','models',CUSTOM_MODEL_NAME, 'export'), \n",
        "    'TFJS_PATH':os.path.join('workspace','training_demo','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
        "    'TFLITE_PATH':os.path.join('workspace','training_demo','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
        "    'PROTOC_PATH':os.path.join('protoc')\n",
        " }\n",
        "\n",
        "files = {\n",
        "    'PIPELINE_CONFIG':os.path.join('workspace','training_demo','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
        "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME),\n",
        "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
        "}\n",
        "\n",
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
        "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
        "\n",
        "# Restore checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-6')).expect_partial()\n",
        "\n",
        "@tf.function\n",
        "def detect_fn(image):\n",
        "    image, shapes = detection_model.preprocess(image)\n",
        "    prediction_dict = detection_model.predict(image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    return detections\n",
        "\n",
        "category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])\n",
        "\n",
        "IMAGE_PATH = os.path.join(paths['IMAGE_PATH'], 'photo.jpg')\n",
        "\n",
        "img = cv2.imread(IMAGE_PATH)\n",
        "image_np = np.array(img)\n",
        "\n",
        "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "num_detections = int(detections.pop('num_detections'))\n",
        "print(num_detections)\n",
        "detections = {key: value[0, :num_detections].numpy()\n",
        "              for key, value in detections.items()}\n",
        "detections['num_detections'] = num_detections\n",
        "\n",
        "\n",
        "# detection_classes should be ints.\n",
        "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "for x in detections['detection_boxes']:\n",
        "  print(x)\n",
        "\n",
        "label_id_offset = 1\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "            image_np_with_detections,\n",
        "            detections['detection_boxes'],\n",
        "            detections['detection_classes']+label_id_offset,\n",
        "            detections['detection_scores'],\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            max_boxes_to_draw=20,\n",
        "            min_score_thresh=.4,\n",
        "            agnostic_mode=False)\n",
        "\n",
        "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![detection](LatexPdf/img/latex/merge.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzBB5ASFKEA9"
      },
      "source": [
        "##Export model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsFX3UL_KIHp"
      },
      "source": [
        ">Copy <code>cp models/research/object_detection/exporter_main_v2.py workspace/training_demo/</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBcDJc2OKyF3"
      },
      "source": [
        ">Export model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e08JtWRK2z5"
      },
      "outputs": [],
      "source": [
        "!python exporter_main_v2.py --input_type image_tensor --pipeline_config_path models/ssd_mobilenet_v2_fpn_keras/pipeline.config --trained_checkpoint_dir models/ssd_mobilenet_v2_fpn_keras/ --output_directory exported_models/EyeTracking_Check_2022_03_05_19-13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5_z5Q5RxIA8"
      },
      "source": [
        "### Export and Convert into .tflite and quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIn1P6lwwiKN"
      },
      "source": [
        "> Copy from <code>Tensorflow1/models/research/objectdetection/export_tflite_graph_tf2.py</code> to <code>workspace/training_demo</code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPZOHwwnwe7x"
      },
      "outputs": [],
      "source": [
        "!python export_tflite_graph_tf2.py --pipeline_config_path=ssd_mobilenet_v2_fpn_keras/pipeline.config --trained_checkpoint_dir=ssd_mobilenet_v2_fpn_keras/ --output_directory=ssd_mobilenet_v2_fpn_keras/tflite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFXsepJaskSt"
      },
      "source": [
        ">Python <code>converter.py</code> script for converting and quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usa3zG2CzW-O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myqUutKlzY0z"
      },
      "outputs": [],
      "source": [
        "saved_model_dir = 'pathtosaved_model/saved_model'\n",
        "\n",
        "# Representative_dataset function\n",
        "def representative_dataset_int():\n",
        "    for _ in range(100):\n",
        "      data = np.random.rand(1, 244, 244, 3)\n",
        "      yield [data.astype(np.uint)]\n",
        "\n",
        "def representative_dataset_f32():\n",
        "    for _ in range(100):\n",
        "      data = np.random.rand(1, 244, 244, 3)\n",
        "      yield [data.astype(np.float32)]\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# BELOW JUST IN CASE FOR CONVERTING FROM FLOAT32 TO UINT8\n",
        "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "# converter.representative_dataset = representative_dataset_f32\n",
        "\n",
        "tflite_model = converter.convert()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9bVzDGepSLG"
      },
      "source": [
        ">Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFsdiJKIzbhU",
        "outputId": "3b8db27b-6291-47b3-e466-e0ebc6a34d9d"
      },
      "outputs": [],
      "source": [
        "# Save the model.\n",
        "with open('converted_model_DFL.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python converter.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yagqWjg7pwlv"
      },
      "source": [
        "# Metadata File\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIAfONnDqa4o"
      },
      "source": [
        ">Tflite requires a metadata file for a correct functioning during Android's operations when Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images. \n",
        "This file is not required when input tensor has type uint8. [Tflite official guide](https://www.tensorflow.org/lite/convert/metadata?hl=en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">We generate a new <code>Tensorflow1/workspace/training_demo/annotations/labelmap.txt</code> file containing for each row our items. In our case, we have only one row for our 'Human eye'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlT84kCKp9_e"
      },
      "outputs": [],
      "source": [
        "from tflite_support.metadata_writers import object_detector\n",
        "from tflite_support.metadata_writers import writer_utils\n",
        "from tflite_support import metadata\n",
        "\n",
        "ObjectDetectorWriter = object_detector.MetadataWriter\n",
        "_MODEL_PATH = \"eyemodel.tflite\"\n",
        "_LABEL_FILE = \"labelmap.txt\"\n",
        "_SAVE_TO_PATH = \"eyemodel_metadata.tflite\"\n",
        "\n",
        "writer = ObjectDetectorWriter.create_for_inference(\n",
        "    writer_utils.load_file(_MODEL_PATH), [127.5], [127.5], [_LABEL_FILE])\n",
        "writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)\n",
        "\n",
        "# Verify the populated metadata and associated files.\n",
        "displayer = metadata.MetadataDisplayer.with_model_file(_SAVE_TO_PATH)\n",
        "print(\"Metadata populated:\")\n",
        "print(displayer.get_metadata_json())\n",
        "print(\"Associated file(s) populated:\")\n",
        "print(displayer.get_packed_associated_file_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT1HT37NrfcR"
      },
      "source": [
        ">Now, we import this new <code>eyemodel_metadata.tflite</code> to our Android App as a Machine Learning [ml](https://github.com/DaniDF/sistemiDigitali2022/tree/master/Android/app/src/main/ml) file."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_custom_object_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
